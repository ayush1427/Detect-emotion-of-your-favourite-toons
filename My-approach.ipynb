{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Copy of 37%-accuracy.ipynb","provenance":[],"authorship_tag":"ABX9TyMQCeNLVp6f3hZrCQJ7Te14"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"is-U6_KI0m_9","colab_type":"code","outputId":"d291ccd3-c24e-424b-d878-080b66508fcd","executionInfo":{"status":"ok","timestamp":1585970668664,"user_tz":-330,"elapsed":43666,"user":{"displayName":"Ayush Kumar","photoUrl":"","userId":"04706745049607145447"}},"colab":{"base_uri":"https://localhost:8080/","height":122}},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n","\n","Enter your authorization code:\n","··········\n","Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"l41rf8jK344y","colab_type":"code","outputId":"f2c1743b-8b91-4db5-e4c8-8ad5649f45bc","executionInfo":{"status":"ok","timestamp":1585970673898,"user_tz":-330,"elapsed":48864,"user":{"displayName":"Ayush Kumar","photoUrl":"","userId":"04706745049607145447"}},"colab":{"base_uri":"https://localhost:8080/","height":292}},"source":["#!pip install keras --upgrade\n","\n","!pip install -U keras\n","\n","# !pip install tensorflow==1.14.0  # After running this, I get some instruction in red color which is\n","                                  # 'RESTART KERNEL' button below red color instr. . Then click on\n","                                  # that button.  \n","\n"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Collecting keras\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ad/fd/6bfe87920d7f4fd475acd28500a42482b6b84479832bdc0fe9e589a60ceb/Keras-2.3.1-py2.py3-none-any.whl (377kB)\n","\r\u001b[K     |▉                               | 10kB 22.0MB/s eta 0:00:01\r\u001b[K     |█▊                              | 20kB 28.3MB/s eta 0:00:01\r\u001b[K     |██▋                             | 30kB 31.7MB/s eta 0:00:01\r\u001b[K     |███▌                            | 40kB 20.8MB/s eta 0:00:01\r\u001b[K     |████▍                           | 51kB 20.2MB/s eta 0:00:01\r\u001b[K     |█████▏                          | 61kB 16.2MB/s eta 0:00:01\r\u001b[K     |██████                          | 71kB 15.9MB/s eta 0:00:01\r\u001b[K     |███████                         | 81kB 16.9MB/s eta 0:00:01\r\u001b[K     |███████▉                        | 92kB 15.8MB/s eta 0:00:01\r\u001b[K     |████████▊                       | 102kB 17.0MB/s eta 0:00:01\r\u001b[K     |█████████▌                      | 112kB 17.0MB/s eta 0:00:01\r\u001b[K     |██████████▍                     | 122kB 17.0MB/s eta 0:00:01\r\u001b[K     |███████████▎                    | 133kB 17.0MB/s eta 0:00:01\r\u001b[K     |████████████▏                   | 143kB 17.0MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 153kB 17.0MB/s eta 0:00:01\r\u001b[K     |█████████████▉                  | 163kB 17.0MB/s eta 0:00:01\r\u001b[K     |██████████████▊                 | 174kB 17.0MB/s eta 0:00:01\r\u001b[K     |███████████████▋                | 184kB 17.0MB/s eta 0:00:01\r\u001b[K     |████████████████▌               | 194kB 17.0MB/s eta 0:00:01\r\u001b[K     |█████████████████▍              | 204kB 17.0MB/s eta 0:00:01\r\u001b[K     |██████████████████▏             | 215kB 17.0MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 225kB 17.0MB/s eta 0:00:01\r\u001b[K     |████████████████████            | 235kB 17.0MB/s eta 0:00:01\r\u001b[K     |████████████████████▉           | 245kB 17.0MB/s eta 0:00:01\r\u001b[K     |█████████████████████▊          | 256kB 17.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████▌         | 266kB 17.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████▍        | 276kB 17.0MB/s eta 0:00:01\r\u001b[K     |████████████████████████▎       | 286kB 17.0MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▏      | 296kB 17.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 307kB 17.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▉     | 317kB 17.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▊    | 327kB 17.0MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▋   | 337kB 17.0MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▌  | 348kB 17.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▍ | 358kB 17.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▏| 368kB 17.0MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 378kB 17.0MB/s \n","\u001b[?25hRequirement already satisfied, skipping upgrade: six>=1.9.0 in /usr/local/lib/python3.6/dist-packages (from keras) (1.12.0)\n","Requirement already satisfied, skipping upgrade: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from keras) (1.1.0)\n","Requirement already satisfied, skipping upgrade: scipy>=0.14 in /usr/local/lib/python3.6/dist-packages (from keras) (1.4.1)\n","Requirement already satisfied, skipping upgrade: pyyaml in /usr/local/lib/python3.6/dist-packages (from keras) (3.13)\n","Requirement already satisfied, skipping upgrade: numpy>=1.9.1 in /usr/local/lib/python3.6/dist-packages (from keras) (1.18.2)\n","Requirement already satisfied, skipping upgrade: keras-applications>=1.0.6 in /usr/local/lib/python3.6/dist-packages (from keras) (1.0.8)\n","Requirement already satisfied, skipping upgrade: h5py in /usr/local/lib/python3.6/dist-packages (from keras) (2.10.0)\n","Installing collected packages: keras\n","  Found existing installation: Keras 2.2.5\n","    Uninstalling Keras-2.2.5:\n","      Successfully uninstalled Keras-2.2.5\n","Successfully installed keras-2.3.1\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"zOGw4RiV0U3I","colab_type":"code","outputId":"b5bb19a4-ec7a-464c-b395-1897ea094f93","executionInfo":{"status":"ok","timestamp":1585970675705,"user_tz":-330,"elapsed":50653,"user":{"displayName":"Ayush Kumar","photoUrl":"","userId":"04706745049607145447"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["import pandas as pd\n","import numpy as np \n","import keras \n","from keras.applications.vgg16 import VGG16\n","from keras.applications.vgg19 import VGG19\n","from keras.applications.mobilenet import MobileNet\n","from keras.applications.resnet import ResNet101\n","#from keras.applications import ResNet50\n","\n","\n","\n","from keras.preprocessing import image\n","from keras.applications.vgg16 import preprocess_input, decode_predictions\n","from keras.layers import Dense, Activation, Flatten, merge,Input\n","from keras.models import Model, Sequential\n","from keras.utils import np_utils\n","from keras import backend as K \n","from keras.layers import *\n","from keras.callbacks import *\n","import os\n","import glob\n","import tensorflow as tf \n","\n","from keras.preprocessing.image import ImageDataGenerator, load_img \n","\n","import warnings\n","warnings.filterwarnings(\"ignore\") "],"execution_count":0,"outputs":[{"output_type":"stream","text":["Using TensorFlow backend.\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"nc4f6s4E05PX","colab_type":"code","outputId":"4fa3d984-23ac-40a9-ab89-08c5bf90f4e6","executionInfo":{"status":"ok","timestamp":1585970772005,"user_tz":-330,"elapsed":146919,"user":{"displayName":"Ayush Kumar","photoUrl":"","userId":"04706745049607145447"}},"colab":{"base_uri":"https://localhost:8080/","height":136}},"source":["# train = pd.read_csv('/kaggle/input/image-classification-dataset-in-the-gala-event/image_auto_tagging/train.csv')\n","train = pd.read_csv('/content/drive/My Drive/Hackerearth/toons-detection/Train.csv')\n","\n","# test  = pd.read_csv('/kaggle/input/image-classification-dataset-in-the-gala-event/image_auto_tagging/test.csv')\n","test  = pd.read_csv('/content/drive/My Drive/Hackerearth/toons-detection/Test.csv')\n","\n","s = train['Emotion'].tolist() \n"," \n","from collections import Counter\n","\n","print(Counter(s).keys()) # equals to list(set(words))\n","print(Counter(s).values()) # counts the elements' frequency\n","print(len(Counter(s).keys()))\n","\n","# tg_dict = {\"Food\":0, \"misc\": 1, \"Attire\": 2,\"Decorationandsignage\":3}\n","\n","tg_dict = {\"Unknown\":0, \"angry\": 1, \"happy\": 2, \"sad\":3, \"surprised\":4} \n","\n","def label_encode(x):\n","    return tg_dict[x]\n","\n","train['Emotion'] = train['Emotion'].apply(label_encode)\n","\n","images = train['Frame_ID'].tolist()\n","classes = train['Emotion'].tolist()\n","\n","features=[]\n","labels=[]\n","# path = '/kaggle/input/image-classification-dataset-in-the-gala-event/image_auto_tagging/Train_Images/'\n","# path = '/content/drive/My Drive/Hackerearth/dataset/Train Images/'\n","\n","path = '/content/drive/My Drive/Hackerearth/toons-detection/train_frames/'\n","\n","for i in range(0,298): \n","  if os.path.isfile(path+str(images[i])):\n","    pic = image.load_img(path+str(images[i]), target_size=(224, 224))\n","    #print(path+str(images[i]))\n","    x = image.img_to_array(pic)\n","    x = np.expand_dims(x, axis=0)\n","    x = preprocess_input(x)\n","    features.append(x)\n","    labels.append(classes[i])\n","  else:\n","    print(path+str(images[i]), 'not present')\n","    \n","npfeatures = np.array(features)\n","print(npfeatures.shape)\n","img_dt = np.rollaxis(npfeatures, 1, 0)\n","print(img_dt.shape)\n","X = img_dt[0]\n","print(X.shape)\n","labels = np.array(labels)\n","# Y = np_utils.to_categorical(labels,4)\n","Y = np_utils.to_categorical(labels,5)\n","\n","print(Y.shape)\n","\n","\n","from sklearn.model_selection import train_test_split\n","from keras.preprocessing.image import ImageDataGenerator"],"execution_count":0,"outputs":[{"output_type":"stream","text":["dict_keys(['happy', 'surprised', 'angry', 'Unknown', 'sad'])\n","dict_values([37, 68, 37, 116, 40])\n","5\n","(298, 1, 224, 224, 3)\n","(1, 298, 224, 224, 3)\n","(298, 224, 224, 3)\n","(298, 5)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"YQBU7Ff-p4aq","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"-7aibSYup4rE","colab_type":"code","outputId":"8fe6133c-4823-4a48-9e22-893f4c5a44ba","executionInfo":{"status":"ok","timestamp":1585970781925,"user_tz":-330,"elapsed":156808,"user":{"displayName":"Ayush Kumar","photoUrl":"","userId":"04706745049607145447"}},"colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["IMAGE_SIZE=[224,224]\n","pretrained_model = MobileNet(weights='imagenet', include_top=False ,input_shape=[*IMAGE_SIZE, 3])\n","\n","# Freeze all the layers\n","for layer in pretrained_model.layers[:]:\n","    layer.trainable = False \n","\n","# Check the trainable status of the individual layers\n","for layer in pretrained_model.layers:\n","    print(layer, layer.trainable)\n","\n","\n","from keras import models\n","from keras import layers\n","from keras import optimizers\n","\n","# Create the model\n","model = models.Sequential()\n","\n","# Add the vgg convolutional base model\n","model.add(pretrained_model) \n","\n","# Add new layers\n","model.add(layers.Flatten())\n","model.add(layers.Dense(220, activation='relu'))\n","model.add(layers.Dropout(0.5))\n","model.add(layers.Dense(220, activation='relu'))\n","model.add(layers.Dropout(0.5))\n","model.add(layers.Dense(5, activation='softmax'))\n","\n","# Show a summary of the model. Check the number of trainable parameters\n","model.summary()"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Downloading data from https://github.com/fchollet/deep-learning-models/releases/download/v0.6/mobilenet_1_0_224_tf_no_top.h5\n","17227776/17225924 [==============================] - 1s 0us/step\n","<keras.engine.input_layer.InputLayer object at 0x7fbee1f1c588> False\n","<keras.layers.convolutional.ZeroPadding2D object at 0x7fbee1f1ca58> False\n","<keras.layers.convolutional.Conv2D object at 0x7fbee1f1cba8> False\n","<keras.layers.normalization.BatchNormalization object at 0x7fbee1f1cc18> False\n","<keras.layers.advanced_activations.ReLU object at 0x7fbee1f1cd30> False\n","<keras.layers.convolutional.DepthwiseConv2D object at 0x7fbee14a3518> False\n","<keras.layers.normalization.BatchNormalization object at 0x7fbee14a3320> False\n","<keras.layers.advanced_activations.ReLU object at 0x7fbed0434dd8> False\n","<keras.layers.convolutional.Conv2D object at 0x7fbed03d8e80> False\n","<keras.layers.normalization.BatchNormalization object at 0x7fbed03e4470> False\n","<keras.layers.advanced_activations.ReLU object at 0x7fbed03f7e10> False\n","<keras.layers.convolutional.ZeroPadding2D object at 0x7fbed03d8ba8> False\n","<keras.layers.convolutional.DepthwiseConv2D object at 0x7fbed03955f8> False\n","<keras.layers.normalization.BatchNormalization object at 0x7fbed03aaba8> False\n","<keras.layers.advanced_activations.ReLU object at 0x7fbed03a6780> False\n","<keras.layers.convolutional.Conv2D object at 0x7fbed03ca278> False\n","<keras.layers.normalization.BatchNormalization object at 0x7fbed0355ba8> False\n","<keras.layers.advanced_activations.ReLU object at 0x7fbed035bc50> False\n","<keras.layers.convolutional.DepthwiseConv2D object at 0x7fbed0374f98> False\n","<keras.layers.normalization.BatchNormalization object at 0x7fbed038ce48> False\n","<keras.layers.advanced_activations.ReLU object at 0x7fbed0320e80> False\n","<keras.layers.convolutional.Conv2D object at 0x7fbed0327c88> False\n","<keras.layers.normalization.BatchNormalization object at 0x7fbed033ba90> False\n","<keras.layers.advanced_activations.ReLU object at 0x7fbed033b5c0> False\n","<keras.layers.convolutional.ZeroPadding2D object at 0x7fbed02d7da0> False\n","<keras.layers.convolutional.DepthwiseConv2D object at 0x7fbed02d7cf8> False\n","<keras.layers.normalization.BatchNormalization object at 0x7fbed02f6898> False\n","<keras.layers.advanced_activations.ReLU object at 0x7fbed0306c18> False\n","<keras.layers.convolutional.Conv2D object at 0x7fbed0309a90> False\n","<keras.layers.normalization.BatchNormalization object at 0x7fbed029d908> False\n","<keras.layers.advanced_activations.ReLU object at 0x7fbed029d438> False\n","<keras.layers.convolutional.DepthwiseConv2D object at 0x7fbed02bdc18> False\n","<keras.layers.normalization.BatchNormalization object at 0x7fbed0251b00> False\n","<keras.layers.advanced_activations.ReLU object at 0x7fbed0265d68> False\n","<keras.layers.convolutional.Conv2D object at 0x7fbed026bef0> False\n","<keras.layers.normalization.BatchNormalization object at 0x7fbed026f4e0> False\n","<keras.layers.advanced_activations.ReLU object at 0x7fbed0281ef0> False\n","<keras.layers.convolutional.ZeroPadding2D object at 0x7fbed021c7f0> False\n","<keras.layers.convolutional.DepthwiseConv2D object at 0x7fbed021c748> False\n","<keras.layers.normalization.BatchNormalization object at 0x7fbed0238d68> False\n","<keras.layers.advanced_activations.ReLU object at 0x7fbed022e940> False\n","<keras.layers.convolutional.Conv2D object at 0x7fbed024ff98> False\n","<keras.layers.normalization.BatchNormalization object at 0x7fbed01d1358> False\n","<keras.layers.advanced_activations.ReLU object at 0x7fbed01e4d68> False\n","<keras.layers.convolutional.DepthwiseConv2D object at 0x7fbed024fcc0> False\n","<keras.layers.normalization.BatchNormalization object at 0x7fbed02006a0> False\n","<keras.layers.advanced_activations.ReLU object at 0x7fbed01a8e80> False\n","<keras.layers.convolutional.Conv2D object at 0x7fbed01aef98> False\n","<keras.layers.normalization.BatchNormalization object at 0x7fbed01c0ef0> False\n","<keras.layers.advanced_activations.ReLU object at 0x7fbed01c7ac8> False\n","<keras.layers.convolutional.DepthwiseConv2D object at 0x7fbed015ee80> False\n","<keras.layers.normalization.BatchNormalization object at 0x7fbed0177da0> False\n","<keras.layers.advanced_activations.ReLU object at 0x7fbed01854a8> False\n","<keras.layers.convolutional.Conv2D object at 0x7fbed0111c50> False\n","<keras.layers.normalization.BatchNormalization object at 0x7fbed0123ac8> False\n","<keras.layers.advanced_activations.ReLU object at 0x7fbed01235f8> False\n","<keras.layers.convolutional.DepthwiseConv2D object at 0x7fbed0143dd8> False\n","<keras.layers.normalization.BatchNormalization object at 0x7fbed00d9b00> False\n","<keras.layers.advanced_activations.ReLU object at 0x7fbed00f1c50> False\n","<keras.layers.convolutional.Conv2D object at 0x7fbed0143da0> False\n","<keras.layers.normalization.BatchNormalization object at 0x7fbed01076a0> False\n","<keras.layers.advanced_activations.ReLU object at 0x7fbed01071d0> False\n","<keras.layers.convolutional.DepthwiseConv2D object at 0x7fbed00a59b0> False\n","<keras.layers.normalization.BatchNormalization object at 0x7fbed00a53c8> False\n","<keras.layers.advanced_activations.ReLU object at 0x7fbed00cf630> False\n","<keras.layers.convolutional.Conv2D object at 0x7fbed0055eb8> False\n","<keras.layers.normalization.BatchNormalization object at 0x7fbed005a278> False\n","<keras.layers.advanced_activations.ReLU object at 0x7fbed006ce10> False\n","<keras.layers.convolutional.DepthwiseConv2D object at 0x7fbed0055940> False\n","<keras.layers.normalization.BatchNormalization object at 0x7fbed00895c0> False\n","<keras.layers.advanced_activations.ReLU object at 0x7fbed002ef60> False\n","<keras.layers.convolutional.Conv2D object at 0x7fbed0036f98> False\n","<keras.layers.normalization.BatchNormalization object at 0x7fbed0048d68> False\n","<keras.layers.advanced_activations.ReLU object at 0x7fbed004dac8> False\n","<keras.layers.convolutional.ZeroPadding2D object at 0x7fbebc7ccda0> False\n","<keras.layers.convolutional.DepthwiseConv2D object at 0x7fbebc7ccc88> False\n","<keras.layers.normalization.BatchNormalization object at 0x7fbebc7edc18> False\n","<keras.layers.advanced_activations.ReLU object at 0x7fbebc7f4c50> False\n","<keras.layers.convolutional.Conv2D object at 0x7fbebc780e10> False\n","<keras.layers.normalization.BatchNormalization object at 0x7fbebc794c88> False\n","<keras.layers.advanced_activations.ReLU object at 0x7fbebc79af28> False\n","<keras.layers.convolutional.DepthwiseConv2D object at 0x7fbebc7b3f98> False\n","<keras.layers.normalization.BatchNormalization object at 0x7fbebc74acc0> False\n","<keras.layers.advanced_activations.ReLU object at 0x7fbebc75ff98> False\n","<keras.layers.convolutional.Conv2D object at 0x7fbebc7b3f28> False\n","<keras.layers.normalization.BatchNormalization object at 0x7fbebc777860> False\n","<keras.layers.advanced_activations.ReLU object at 0x7fbebc777390> False\n","Model: \"sequential_1\"\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","mobilenet_1.00_224 (Model)   (None, 7, 7, 1024)        3228864   \n","_________________________________________________________________\n","flatten_1 (Flatten)          (None, 50176)             0         \n","_________________________________________________________________\n","dense_1 (Dense)              (None, 220)               11038940  \n","_________________________________________________________________\n","dropout_1 (Dropout)          (None, 220)               0         \n","_________________________________________________________________\n","dense_2 (Dense)              (None, 220)               48620     \n","_________________________________________________________________\n","dropout_2 (Dropout)          (None, 220)               0         \n","_________________________________________________________________\n","dense_3 (Dense)              (None, 5)                 1105      \n","=================================================================\n","Total params: 14,317,529\n","Trainable params: 11,088,665\n","Non-trainable params: 3,228,864\n","_________________________________________________________________\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"JUWelwC0rRzz","colab_type":"code","outputId":"e6e6e511-2d35-4204-e23e-5ff6219f71f5","executionInfo":{"status":"ok","timestamp":1585970809600,"user_tz":-330,"elapsed":184464,"user":{"displayName":"Ayush Kumar","photoUrl":"","userId":"04706745049607145447"}},"colab":{"base_uri":"https://localhost:8080/","height":731}},"source":["#model.compile(optimizer = optimizers.Adagrad(learning_rate=0.01), loss='categorical_crossentropy') \n","# keras.optimizers.Adagrad(learning_rate=0.01)\n","model.compile(optimizer = optimizers.RMSprop(lr=1e-4) , loss='categorical_crossentropy', metrics=['accuracy']) \n","\n","model.fit(X, Y, batch_size=32, epochs=20, validation_split=.1)  "],"execution_count":0,"outputs":[{"output_type":"stream","text":["Train on 268 samples, validate on 30 samples\n","Epoch 1/20\n","268/268 [==============================] - 9s 33ms/step - loss: 4.0797 - val_loss: 0.7212\n","Epoch 2/20\n","268/268 [==============================] - 1s 3ms/step - loss: 2.3346 - val_loss: 0.6332\n","Epoch 3/20\n","268/268 [==============================] - 1s 3ms/step - loss: 1.7866 - val_loss: 0.7976\n","Epoch 4/20\n","268/268 [==============================] - 1s 3ms/step - loss: 1.3148 - val_loss: 0.6231\n","Epoch 5/20\n","268/268 [==============================] - 1s 3ms/step - loss: 1.0737 - val_loss: 0.7649\n","Epoch 6/20\n","268/268 [==============================] - 1s 3ms/step - loss: 1.0821 - val_loss: 0.5779\n","Epoch 7/20\n","268/268 [==============================] - 1s 3ms/step - loss: 0.9098 - val_loss: 0.5262\n","Epoch 8/20\n","268/268 [==============================] - 1s 3ms/step - loss: 0.9273 - val_loss: 0.7467\n","Epoch 9/20\n","268/268 [==============================] - 1s 3ms/step - loss: 0.7798 - val_loss: 0.6550\n","Epoch 10/20\n","268/268 [==============================] - 1s 3ms/step - loss: 0.8172 - val_loss: 0.5490\n","Epoch 11/20\n","268/268 [==============================] - 1s 3ms/step - loss: 0.7598 - val_loss: 0.5657\n","Epoch 12/20\n","268/268 [==============================] - 1s 3ms/step - loss: 0.8738 - val_loss: 0.4133\n","Epoch 13/20\n","268/268 [==============================] - 1s 3ms/step - loss: 0.8385 - val_loss: 0.5279\n","Epoch 14/20\n","268/268 [==============================] - 1s 3ms/step - loss: 0.6883 - val_loss: 0.4739\n","Epoch 15/20\n","268/268 [==============================] - 1s 3ms/step - loss: 0.6830 - val_loss: 0.4286\n","Epoch 16/20\n","268/268 [==============================] - 1s 3ms/step - loss: 0.6434 - val_loss: 0.5361\n","Epoch 17/20\n","268/268 [==============================] - 1s 3ms/step - loss: 0.4744 - val_loss: 0.5789\n","Epoch 18/20\n","268/268 [==============================] - 1s 3ms/step - loss: 0.5866 - val_loss: 0.5057\n","Epoch 19/20\n","268/268 [==============================] - 1s 3ms/step - loss: 0.5274 - val_loss: 0.4983\n","Epoch 20/20\n","268/268 [==============================] - 1s 3ms/step - loss: 0.4226 - val_loss: 0.4056\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["<keras.callbacks.callbacks.History at 0x7fbeaf9a5cc0>"]},"metadata":{"tags":[]},"execution_count":6}]},{"cell_type":"code","metadata":{"id":"PIAaacuNp46E","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"yDEcmMUt05UB","colab_type":"code","colab":{}},"source":["images_test = test['Frame_ID'].tolist()\n","test_features=[] \n","# path_test = '/kaggle/input/image-classification-dataset-in-the-gala-event/image_auto_tagging/Test_Images/'\n","path_test = '/content/drive/My Drive/Hackerearth/toons-detection/test_frames/' \n","\n","for i in range(0,186): \n","  if os.path.isfile(path_test+str(images_test[i])):\n","    pic = image.load_img(path_test+str(images_test[i]), target_size=(224, 224))\n","    #print(path+str(images[i]))\n","    x = image.img_to_array(pic)  \n","    x = np.expand_dims(x, axis=0) \n","    x = preprocess_input(x)\n","    test_features.append(x)\n","  else:\n","    print(path_test+str(images[i]), 'not present')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"HTuMfbHm0kTw","colab_type":"code","outputId":"2eba45d7-6e79-44cf-bacf-7d29342d6dc0","executionInfo":{"status":"ok","timestamp":1585970890290,"user_tz":-330,"elapsed":265085,"user":{"displayName":"Ayush Kumar","photoUrl":"","userId":"04706745049607145447"}},"colab":{"base_uri":"https://localhost:8080/","height":68}},"source":["test_features = np.array(test_features) \n","print(test_features.shape)\n","test_features = np.rollaxis(test_features, 1, 0)\n","print(test_features.shape)\n","X_test = test_features[0]\n","print(X_test.shape)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["(186, 1, 224, 224, 3)\n","(1, 186, 224, 224, 3)\n","(186, 224, 224, 3)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"9z0DTU5V0kYI","colab_type":"code","colab":{}},"source":["preds = model.predict(X_test) "],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"AkRsxsYG0kbC","colab_type":"code","colab":{}},"source":["predictions=[] \n","for i in preds:\n","    predictions.append(np.argmax(i))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"WDe4EjbQ0kRq","colab_type":"code","colab":{}},"source":["test['Emotion'] = predictions "],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"_rTEZmUK5GSb","colab_type":"code","colab":{}},"source":["gt_dict = dict((v,k) for k,v in tg_dict.items())\n","\n","def inverse_encode(x):\n","    return gt_dict[x]\n","\n","test['Emotion'] = test['Emotion'].apply(inverse_encode)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"5XMcwkmt5GX-","colab_type":"code","outputId":"0ba41462-adb5-4e96-e516-8db542b0e17a","executionInfo":{"status":"ok","timestamp":1585970892232,"user_tz":-330,"elapsed":266924,"user":{"displayName":"Ayush Kumar","photoUrl":"","userId":"04706745049607145447"}},"colab":{"base_uri":"https://localhost:8080/","height":80}},"source":["test.head(1)"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Frame_ID</th>\n","      <th>Emotion</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>test0.jpg</td>\n","      <td>angry</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["    Frame_ID Emotion\n","0  test0.jpg   angry"]},"metadata":{"tags":[]},"execution_count":13}]},{"cell_type":"code","metadata":{"id":"A0YarAqU5GVd","colab_type":"code","colab":{}},"source":["test.to_csv('/content/drive/My Drive/Hackerearth/toons-detection/Submission15.csv',header=True,index = None) "],"execution_count":0,"outputs":[]}]}